{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ae76d3-c44a-44ac-a9e2-e1f2d4bd46a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "import requests\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def load_game_dates():\n",
    "    \"\"\"Load game dates data from GitHub\"\"\"\n",
    "    url = 'https://raw.githubusercontent.com/gabriel1200/shot_data/refs/heads/master/game_dates.csv'\n",
    "    try:\n",
    "        game_dates_df = pd.read_csv(url)\n",
    "        print(f\"Loaded game dates data: {len(game_dates_df)} rows\")\n",
    "        \n",
    "        # Convert GAME_ID and TEAM_ID to strings for consistent matching\n",
    "        game_dates_df['GAME_ID'] = game_dates_df['GAME_ID'].astype(str)\n",
    "        game_dates_df['TEAM_ID'] = game_dates_df['TEAM_ID'].astype(str)\n",
    "        \n",
    "        return game_dates_df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading game dates data: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_team_acronym_from_id(team_id):\n",
    "    \"\"\"Convert team ID to team acronym using the provided dictionary\"\"\"\n",
    "    team_dict = {\n",
    "        '1610612760': 'OKC', '1610612749': 'MIL', '1610612758': 'SAC', '1610612747': 'LAL',\n",
    "        '1610612738': 'BOS', '1610612743': 'DEN', '1610612750': 'MIN', '1610612752': 'NYK',\n",
    "        '1610612756': 'PHX', '1610612753': 'ORL', '1610612766': 'CHA', '1610612739': 'CLE',\n",
    "        '1610612746': 'LAC', '1610612737': 'ATL', '1610612748': 'MIA', '1610612742': 'DAL',\n",
    "        '1610612765': 'DET', '1610612763': 'MEM', '1610612761': 'TOR', '1610612741': 'CHI',\n",
    "        '1610612754': 'IND', '1610612759': 'SAS', '1610612745': 'HOU', '1610612751': 'BKN',\n",
    "        '1610612764': 'WAS', '1610612744': 'GSW', '1610612755': 'PHI', '1610612762': 'UTA',\n",
    "        '1610612757': 'POR', '1610612740': 'NOP'\n",
    "    }\n",
    "    \n",
    "    team_id_str = str(int(team_id)) if pd.notna(team_id) else None\n",
    "    return team_dict.get(team_id_str, f'UNKNOWN_{team_id_str}')\n",
    "\n",
    "def determine_game_type(game_id):\n",
    "    \"\"\"Determine if game is regular season (rs) or playoffs (ps) based on game ID\"\"\"\n",
    "    game_id_str = str(game_id)\n",
    "    if game_id_str.startswith('4'):\n",
    "        return 'ps'  # Playoffs\n",
    "    elif game_id_str.startswith('2'):\n",
    "        return 'rs'  # Regular season\n",
    "    else:\n",
    "        return 'unknown'\n",
    "\n",
    "def get_teams_in_game(df):\n",
    "    \"\"\"Get the two teams that participated in the game\"\"\"\n",
    "    teams = df['teamId'].dropna().unique()\n",
    "    # Filter out any invalid team IDs and convert to strings\n",
    "    valid_teams = []\n",
    "    for team in teams:\n",
    "        try:\n",
    "            team_str = str(int(team))\n",
    "            valid_teams.append(team_str)\n",
    "        except (ValueError, TypeError):\n",
    "            continue\n",
    "    \n",
    "    return valid_teams\n",
    "\n",
    "def merge_game_dates_data(game_df, game_dates_df):\n",
    "    \"\"\"Merge game dates data with the game DataFrame\"\"\"\n",
    "    if game_dates_df is None:\n",
    "        print(\"Warning: No game dates data available for merging\")\n",
    "        return game_df\n",
    "    \n",
    "    # Convert game_df columns to strings for consistent matching\n",
    "    game_df['game_id_str'] = game_df['game_id'].astype(str)\n",
    "    game_df['teamId_str'] = game_df['teamId'].astype(int).astype(str)\n",
    "    \n",
    "    # Create merge key\n",
    "    game_df['merge_key'] = game_df['game_id_str'] + '_' + game_df['teamId_str']\n",
    "    game_dates_df['merge_key'] = game_dates_df['GAME_ID'] + '_' + game_dates_df['TEAM_ID']\n",
    "    \n",
    "    # Merge the data\n",
    "    merged_df = game_df.merge(\n",
    "        game_dates_df[['merge_key', 'date', 'playoffs', 'team', 'opp_team']], \n",
    "        on='merge_key', \n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Clean up temporary columns\n",
    "    merged_df = merged_df.drop(['merge_key', 'game_id_str', 'teamId_str'], axis=1)\n",
    "    \n",
    "    # Report merge statistics\n",
    "    matched_rows = merged_df['date'].notna().sum()\n",
    "    total_rows = len(merged_df)\n",
    "    print(f\"    Merged game dates: {matched_rows}/{total_rows} rows matched\")\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "def collect_and_organize_data():\n",
    "    \"\"\"Main function to collect and organize scraped PBP data\"\"\"\n",
    "    \n",
    "    # Load game dates data first\n",
    "    game_dates_df = load_game_dates()\n",
    "    \n",
    "    # Create output directories\n",
    "    os.makedirs('organized_data', exist_ok=True)\n",
    "    os.makedirs('organized_data/regular_season', exist_ok=True)\n",
    "    os.makedirs('organized_data/playoffs', exist_ok=True)\n",
    "    \n",
    "    # Get all CSV files from pbp_data directory\n",
    "    pbp_files = glob.glob('pbp_data/*.csv')\n",
    "    \n",
    "    if not pbp_files:\n",
    "        print(\"No PBP data files found in pbp_data directory!\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(pbp_files)} PBP data files\")\n",
    "    \n",
    "    # Dictionary to store complete game data by team, year, and game type\n",
    "    # Structure: team_games[team_acronym][year][game_type] = [list of complete game DataFrames]\n",
    "    team_games = defaultdict(lambda: defaultdict(lambda: defaultdict(list)))\n",
    "    \n",
    "    # Process each file (each file represents one complete game)\n",
    "    processed_files = 0\n",
    "    skipped_files = 0\n",
    "    \n",
    "    for file_path in pbp_files:\n",
    "        try:\n",
    "            # Extract year from filename (format: YYYY_GAMEID.csv)\n",
    "            filename = os.path.basename(file_path)\n",
    "            if '_' not in filename:\n",
    "                print(f\"Skipping file with unexpected format: {filename}\")\n",
    "                skipped_files += 1\n",
    "                continue\n",
    "            \n",
    "            year = filename.split('_')[0]\n",
    "            \n",
    "            # Read the complete game CSV file\n",
    "            game_df = pd.read_csv(file_path)\n",
    "            \n",
    "            if game_df.empty:\n",
    "                print(f\"Skipping empty file: {filename}\")\n",
    "                skipped_files += 1\n",
    "                continue\n",
    "            \n",
    "            # Check if required columns exist\n",
    "            required_columns = ['teamId', 'game_id']\n",
    "            missing_columns = [col for col in required_columns if col not in game_df.columns]\n",
    "            if missing_columns:\n",
    "                print(f\"Skipping file {filename} - missing columns: {missing_columns}\")\n",
    "                skipped_files += 1\n",
    "                continue\n",
    "            \n",
    "            # Merge with game dates data\n",
    "            game_df = merge_game_dates_data(game_df, game_dates_df)\n",
    "            \n",
    "            # Get the teams that participated in this game\n",
    "            teams_in_game = get_teams_in_game(game_df)\n",
    "            \n",
    "            if len(teams_in_game) == 0:\n",
    "                print(f\"No valid team IDs found in {filename}\")\n",
    "                skipped_files += 1\n",
    "                continue\n",
    "            \n",
    "            # Determine game type from game_id\n",
    "            game_ids = game_df['game_id'].dropna().unique()\n",
    "            if len(game_ids) == 0:\n",
    "                print(f\"No valid game IDs found in {filename}\")\n",
    "                skipped_files += 1\n",
    "                continue\n",
    "            \n",
    "            # Use first game_id to determine game type (should be consistent within file)\n",
    "            game_type = determine_game_type(game_ids[0])\n",
    "            \n",
    "            if game_type == 'unknown':\n",
    "                print(f\"Unknown game type for {filename} (game_id: {game_ids[0]})\")\n",
    "                skipped_files += 1\n",
    "                continue\n",
    "            \n",
    "            # Add this complete game to each participating team's collection\n",
    "            for team_id in teams_in_game:\n",
    "                team_acronym = get_team_acronym_from_id(team_id)\n",
    "                \n",
    "                # Add the complete game DataFrame (includes both teams' actions)\n",
    "                team_games[team_acronym][year][game_type].append(game_df.copy())\n",
    "            \n",
    "            processed_files += 1\n",
    "            \n",
    "            if processed_files % 100 == 0:\n",
    "                print(f\"Processed {processed_files} files...\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_path}: {e}\")\n",
    "            skipped_files += 1\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\nProcessed {processed_files} files, skipped {skipped_files} files\")\n",
    "    \n",
    "    # Combine and save data for each team/year/game_type combination\n",
    "    total_files_created = 0\n",
    "    \n",
    "    for team_acronym in team_games:\n",
    "        for year in team_games[team_acronym]:\n",
    "            for game_type in team_games[team_acronym][year]:\n",
    "                # Combine all complete games for this team/year/game_type\n",
    "                combined_games_df = pd.concat(team_games[team_acronym][year][game_type], ignore_index=True)\n",
    "                \n",
    "                # Sort by game_id and actionNumber for consistent ordering\n",
    "                combined_games_df = combined_games_df.sort_values(['game_id', 'actionNumber'])\n",
    "                \n",
    "                # Create filename\n",
    "                game_type_folder = 'regular_season' if game_type == 'rs' else 'playoffs'\n",
    "                filename = f\"organized_data/{game_type_folder}/{team_acronym}_{year}_{game_type}.csv\"\n",
    "                filename2 = f\"organized_data/{game_type_folder}/{team_acronym}_{year}_{game_type}.parquet\"\n",
    "                \n",
    "                # Save to CSV\n",
    "                combined_games_df.to_csv(filename, index=False)\n",
    "                combined_games_df.to_parquet(filename2,index=False)\n",
    "                # Count unique games for this team\n",
    "                unique_games = combined_games_df['game_id'].nunique()\n",
    "                \n",
    "                # Count games with date information\n",
    "                games_with_dates = combined_games_df.groupby('game_id')['date'].first().notna().sum()\n",
    "                \n",
    "                total_files_created += 1\n",
    "                print(f\"Created: {filename} ({len(combined_games_df)} rows, {unique_games} games, {games_with_dates} games with dates)\")\n",
    "    \n",
    "    print(f\"\\nData organization complete!\")\n",
    "    print(f\"Total files created: {total_files_created}\")\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\nSummary by team:\")\n",
    "    for team_acronym in sorted(team_games.keys()):\n",
    "        years = sorted(team_games[team_acronym].keys())\n",
    "        rs_years = []\n",
    "        ps_years = []\n",
    "        \n",
    "        for year in years:\n",
    "            if 'rs' in team_games[team_acronym][year]:\n",
    "                rs_games = len(team_games[team_acronym][year]['rs'])\n",
    "                rs_years.append(f\"{year}({rs_games})\")\n",
    "            if 'ps' in team_games[team_acronym][year]:\n",
    "                ps_games = len(team_games[team_acronym][year]['ps'])\n",
    "                ps_years.append(f\"{year}({ps_games})\")\n",
    "        \n",
    "        print(f\"  {team_acronym}: RS: {rs_years}, PS: {ps_years}\")\n",
    "\n",
    "def verify_organized_data():\n",
    "    \"\"\"Verify the organized data files\"\"\"\n",
    "    print(\"\\nVerifying organized data files...\")\n",
    "    \n",
    "    rs_files = glob.glob('organized_data/regular_season/*.csv')\n",
    "    ps_files = glob.glob('organized_data/playoffs/*.csv')\n",
    "    \n",
    "    print(f\"Regular season files: {len(rs_files)}\")\n",
    "    print(f\"Playoff files: {len(ps_files)}\")\n",
    "    \n",
    "    # Sample a few files to verify structure\n",
    "    sample_files = (rs_files + ps_files)[:5]\n",
    "    \n",
    "    for file_path in sample_files:\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            filename = os.path.basename(file_path)\n",
    "            \n",
    "            # Get team info\n",
    "            teams_in_data = df['teamId'].dropna().unique()\n",
    "            team_acronyms = [get_team_acronym_from_id(tid) for tid in teams_in_data]\n",
    "            unique_games = df['game_id'].nunique()\n",
    "            \n",
    "            # Check for new columns\n",
    "            has_date = 'date' in df.columns\n",
    "            has_playoffs = 'playoffs' in df.columns\n",
    "            has_team = 'team' in df.columns\n",
    "            has_opp_team = 'opp_team' in df.columns\n",
    "            \n",
    "            # Count rows with date information\n",
    "            rows_with_dates = df['date'].notna().sum() if has_date else 0\n",
    "            \n",
    "            print(f\"  {filename}: {len(df)} rows, {unique_games} games, teams: {team_acronyms}\")\n",
    "            print(f\"    New columns - date: {has_date}({rows_with_dates}), playoffs: {has_playoffs}, team: {has_team}, opp_team: {has_opp_team}\")\n",
    "            \n",
    "            # Check game type consistency\n",
    "            game_ids = df['game_id'].unique()\n",
    "            game_types = [determine_game_type(gid) for gid in game_ids]\n",
    "            unique_game_types = set(game_types)\n",
    "            \n",
    "            if len(unique_game_types) > 1:\n",
    "                print(f\"    WARNING: Mixed game types found: {unique_game_types}\")\n",
    "            \n",
    "            # Verify both teams' actions are present\n",
    "            if len(teams_in_data) < 2:\n",
    "                print(f\"    WARNING: Expected 2 teams, found {len(teams_in_data)}\")\n",
    "            \n",
    "            # Show sample of new data\n",
    "            if has_date and rows_with_dates > 0:\n",
    "                sample_row = df[df['date'].notna()].iloc[0]\n",
    "                print(f\"    Sample: date={sample_row.get('date', 'N/A')}, team={sample_row.get('team', 'N/A')}, opp_team={sample_row.get('opp_team', 'N/A')}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    ERROR reading {file_path}: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting NBA PBP data collection and organization...\")\n",
    "    collect_and_organize_data()\n",
    "    verify_organized_data()\n",
    "    print(\"\\nProcess complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf6fc4f-dc95-410a-9803-cc6269792a23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
