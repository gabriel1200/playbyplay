{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8573f9c-4100-4b02-a93e-f587473ef0c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 435\u001b[39m\n\u001b[32m    432\u001b[39m     asyncio.run(main_async())\n\u001b[32m    434\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m435\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 432\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    430\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmain\u001b[39m():\n\u001b[32m    431\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Entry point that runs the async main function\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m432\u001b[39m     \u001b[43masyncio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/asyncio/runners.py:190\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(main, debug, loop_factory)\u001b[39m\n\u001b[32m    161\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[32m    162\u001b[39m \n\u001b[32m    163\u001b[39m \u001b[33;03mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    186\u001b[39m \u001b[33;03m    asyncio.run(main())\u001b[39;00m\n\u001b[32m    187\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    188\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m events._get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    189\u001b[39m     \u001b[38;5;66;03m# fail fast with short traceback\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    191\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    193\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Runner(debug=debug, loop_factory=loop_factory) \u001b[38;5;28;01mas\u001b[39;00m runner:\n\u001b[32m    194\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m runner.run(main)\n",
      "\u001b[31mRuntimeError\u001b[39m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from itertools import chain\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import glob\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load master record\n",
    "record = pd.read_csv('master_record.csv')\n",
    "record = record[['GAME_ID','year']]\n",
    "record.drop_duplicates(inplace=True)\n",
    "\n",
    "def get_existing_games():\n",
    "    \"\"\"Get list of already scraped games from existing CSV files\"\"\"\n",
    "    existing_games = set()\n",
    "    \n",
    "    if os.path.exists('pbp_data'):\n",
    "        pbp_files = glob.glob('pbp_data/*.csv')\n",
    "        for file_path in pbp_files:\n",
    "            filename = os.path.basename(file_path)\n",
    "            if '_' in filename:\n",
    "                parts = filename.replace('.csv', '').split('_')\n",
    "                if len(parts) >= 2:\n",
    "                    game_id = '_'.join(parts[1:])\n",
    "                    existing_games.add(game_id)\n",
    "    \n",
    "    print(f\"Found {len(existing_games)} already scraped games\")\n",
    "    return existing_games\n",
    "\n",
    "def get_player_name(play):\n",
    "    \"\"\"Extract player name from play data\"\"\"\n",
    "    if 'playerName' in play and play['playerName']:\n",
    "        return play['playerName']\n",
    "    elif 'playerNameI' in play and play['playerNameI']:\n",
    "        return play['playerNameI']\n",
    "    return ''\n",
    "\n",
    "def is_field_goal(play):\n",
    "    \"\"\"Check if play is a field goal attempt\"\"\"\n",
    "    return play.get('actionType') in ['2pt', '3pt']\n",
    "\n",
    "async def fetch_json_async(session, url, game_id, data_type=\"PBP\"):\n",
    "    \"\"\"Async function to fetch JSON data\"\"\"\n",
    "    try:\n",
    "        async with session.get(url, timeout=aiohttp.ClientTimeout(total=30)) as response:\n",
    "            if response.status == 200:\n",
    "                return await response.json()\n",
    "            else:\n",
    "                print(f\"HTTP {response.status} for {data_type} game {game_id}\")\n",
    "                return None\n",
    "    except asyncio.TimeoutError:\n",
    "        print(f\"Timeout fetching {data_type} data for game {game_id}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {data_type} data for game {game_id}: {e}\")\n",
    "        return None\n",
    "\n",
    "async def get_game_data_async(session, game_id):\n",
    "    \"\"\"Fetch both PBP and boxscore data concurrently\"\"\"\n",
    "    pbp_url = f\"https://cdn.nba.com/static/json/liveData/playbyplay/playbyplay_00{game_id}.json\"\n",
    "    boxscore_url = f\"https://cdn.nba.com/static/json/liveData/boxscore/boxscore_00{game_id}.json\"\n",
    "    \n",
    "    # Fetch both URLs concurrently\n",
    "    pbp_task = fetch_json_async(session, pbp_url, game_id, \"PBP\")\n",
    "    boxscore_task = fetch_json_async(session, boxscore_url, game_id, \"Boxscore\")\n",
    "    \n",
    "    pbp_data, boxscore_data = await asyncio.gather(pbp_task, boxscore_task)\n",
    "    \n",
    "    return pbp_data, boxscore_data\n",
    "\n",
    "def parse_iso_clock(clock_str):\n",
    "    \"\"\"Convert ISO 8601 duration format (PTXMYS) to minutes and seconds\"\"\"\n",
    "    if not clock_str or pd.isna(clock_str):\n",
    "        return 0, 0\n",
    "    match = re.match(r'PT(?:(\\d+)M)?(?:(\\d+(?:\\.\\d+)?)S)?', str(clock_str))\n",
    "    if match:\n",
    "        minutes = int(match.group(1)) if match.group(1) else 0\n",
    "        seconds = float(match.group(2)) if match.group(2) else 0.0\n",
    "        return minutes, int(seconds)\n",
    "    return 0, 0\n",
    "\n",
    "def extract_starters_from_boxscore(boxscore_data):\n",
    "    \"\"\"Extract starting lineups from boxscore data\"\"\"\n",
    "    try:\n",
    "        home_starters = []\n",
    "        away_starters = []\n",
    "        \n",
    "        home_players = boxscore_data['game']['homeTeam']['players']\n",
    "        away_players = boxscore_data['game']['awayTeam']['players']\n",
    "        \n",
    "        for player in home_players:\n",
    "            if player.get('starter') == '1':\n",
    "                home_starters.append(str(player['personId']))\n",
    "        \n",
    "        for player in away_players:\n",
    "            if player.get('starter') == '1':\n",
    "                away_starters.append(str(player['personId']))\n",
    "        \n",
    "        all_starters = home_starters + away_starters\n",
    "        return '|'.join(all_starters)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting starters from boxscore: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def process_pbp_data(pbp_data, boxscore_data, game_id):\n",
    "    \"\"\"Process play-by-play data - optimized version\"\"\"\n",
    "    if not pbp_data or 'game' not in pbp_data or 'actions' not in pbp_data['game']:\n",
    "        return [], {}\n",
    "    \n",
    "    # Extract starters\n",
    "    starters_on = extract_starters_from_boxscore(boxscore_data) if boxscore_data else \"\"\n",
    "    \n",
    "    # Create DataFrame from actions\n",
    "    pbp_df = pd.DataFrame(pbp_data['game']['actions'])\n",
    "    \n",
    "    if pbp_df.empty:\n",
    "        return [], {}\n",
    "    \n",
    "    # Filter for relevant events\n",
    "    events = ['2pt', 'rebound', '3pt', 'turnover', 'steal', 'foul', 'freethrow', 'timeout', 'substitution', 'block']\n",
    "    pbp_df = pbp_df[pbp_df.actionType.isin(events)]\n",
    "    pbp_df = pbp_df.replace({np.nan: None})\n",
    "\n",
    "    if pbp_df.empty:\n",
    "        return [], {}\n",
    "\n",
    "    # Vectorized clock parsing\n",
    "    clock_data = pbp_df['clock'].apply(lambda x: pd.Series(parse_iso_clock(x)))\n",
    "    pbp_df[['clock_minutes', 'clock_seconds']] = clock_data\n",
    "    pbp_df['clock_display'] = pbp_df.apply(\n",
    "        lambda row: f\"{int(row['clock_minutes']):02}:{int(row['clock_seconds']):02}\", axis=1\n",
    "    )\n",
    "    pbp_df['game_clock'] = pbp_df.apply(\n",
    "        lambda row: f\"Q{row['period']} {row['clock_display']}\", axis=1\n",
    "    )\n",
    "    pbp_df['minutes_left_in_game'] = (\n",
    "        (4 - (pbp_df['period'] - 1)) * 12 - \n",
    "        (12 - pbp_df['clock_minutes']) - \n",
    "        (pbp_df['clock_seconds'] / 60)\n",
    "    )\n",
    "\n",
    "    # Compute next action fields\n",
    "    pbp_df['next_actionType'] = pbp_df['actionType'].shift(-1)\n",
    "    pbp_df['next_shotResult'] = pbp_df['shotResult'].shift(-1)\n",
    "\n",
    "    processed_data = []\n",
    "    prev_action_type = None\n",
    "    prev_shot_result = None\n",
    "    team_lineups = defaultdict(set)\n",
    "\n",
    "    players_on_list = starters_on.split('|') if starters_on else []\n",
    "\n",
    "    # Process each play\n",
    "    for idx, play in pbp_df.iterrows():\n",
    "        # Previous and next action logic (unchanged)\n",
    "        previous_action = None\n",
    "        if prev_action_type:\n",
    "            if prev_action_type in ['2pt', '3pt']:\n",
    "                if prev_shot_result == 'Made':\n",
    "                    previous_action = f\"{prev_action_type} made\"\n",
    "                elif prev_shot_result == 'Missed':\n",
    "                    previous_action = f\"{prev_action_type} missed\"\n",
    "                else:\n",
    "                    previous_action = prev_action_type\n",
    "            else:\n",
    "                previous_action = prev_action_type\n",
    "\n",
    "        next_action = None\n",
    "        next_action_type = play.get('next_actionType')\n",
    "        next_shot_result = play.get('next_shotResult')\n",
    "        if next_action_type:\n",
    "            if next_action_type in ['2pt', '3pt']:\n",
    "                if next_shot_result == 'Made':\n",
    "                    next_action = f\"{next_action_type} made\"\n",
    "                elif next_shot_result == 'Missed':\n",
    "                    next_action = f\"{next_action_type} missed\"\n",
    "                else:\n",
    "                    next_action = next_action_type\n",
    "            else:\n",
    "                next_action = next_action_type\n",
    "\n",
    "        # Handle substitutions\n",
    "        if play.get('actionType') == 'substitution':\n",
    "            person_id = str(play.get('personId'))\n",
    "            sub_type = play.get('subType')\n",
    "            if sub_type == 'out' and person_id in players_on_list:\n",
    "                players_on_list.remove(person_id)\n",
    "            elif sub_type == 'in' and person_id not in players_on_list:\n",
    "                players_on_list.append(person_id)\n",
    "\n",
    "        person_id = play.get('personId', None)\n",
    "        assist_id = play.get('assistPersonId', None)\n",
    "        formatted_players_on = '|'.join(sorted(players_on_list))\n",
    "        \n",
    "        team_id = play.get('teamId')\n",
    "        if team_id and formatted_players_on:\n",
    "            team_lineups[team_id].add(formatted_players_on)\n",
    "\n",
    "        play_dict = {\n",
    "            'period': play.get('period', 0),\n",
    "            'clock': play.get('clock', ''),\n",
    "            'clock_display': play.get('clock_display', ''),\n",
    "            'game_clock': play.get('game_clock', ''),\n",
    "            'minutes_left_in_game': play.get('minutes_left_in_game', 0),\n",
    "            'actionNumber': play.get('actionNumber', ''),\n",
    "            'actionType': play.get('actionType', ''),\n",
    "            'description': play.get('description', ''),\n",
    "            'qualifier': play.get('qualifiers', []),\n",
    "            'playerName': get_player_name(play),\n",
    "            'scoreHome': play.get('scoreHome', 0),\n",
    "            'scoreAway': play.get('scoreAway', 0),\n",
    "            'shotResult': play.get('shotResult', ''),\n",
    "            'isFieldGoal': is_field_goal(play),\n",
    "            'assisted': assist_id is not None,\n",
    "            'person_id': person_id,\n",
    "            'assister_id': assist_id,\n",
    "            'previous_action': previous_action,\n",
    "            'next_action': next_action,\n",
    "            'foulDrawnPersonId': play.get('foulDrawnPersonId', ''),\n",
    "            'stealPersonId': play.get('stealPersonId', ''),\n",
    "            'blockPersonId': play.get('blockPersonId', ''),\n",
    "            'players_on': formatted_players_on,\n",
    "            'teamId': team_id,\n",
    "            'game_id': game_id\n",
    "        }\n",
    "\n",
    "        processed_data.append(play_dict)\n",
    "        \n",
    "        prev_action_type = play.get('actionType', '')\n",
    "        prev_shot_result = play.get('shotResult', '')\n",
    "\n",
    "    return processed_data, team_lineups\n",
    "\n",
    "def verify_existing_file(game_id, year):\n",
    "    \"\"\"Verify that an existing file contains valid data\"\"\"\n",
    "    filename = f\"pbp_data/{year}_{game_id}.csv\"\n",
    "    \n",
    "    try:\n",
    "        if not os.path.exists(filename):\n",
    "            return False\n",
    "            \n",
    "        df = pd.read_csv(filename)\n",
    "        \n",
    "        if df.empty:\n",
    "            return False\n",
    "            \n",
    "        required_columns = ['actionType', 'game_id', 'period']\n",
    "        missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "        if missing_columns:\n",
    "            return False\n",
    "            \n",
    "        if 'game_id' in df.columns and not str(df['game_id'].iloc[0]) == str(game_id):\n",
    "            return False\n",
    "            \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        return False\n",
    "\n",
    "async def process_game_batch(session, game_batch, all_team_lineups):\n",
    "    \"\"\"Process a batch of games concurrently\"\"\"\n",
    "    batch_results = []\n",
    "    \n",
    "    # Fetch all game data concurrently\n",
    "    tasks = [get_game_data_async(session, game_id) for game_id, year in game_batch]\n",
    "    game_data_results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "    \n",
    "    # Process each game's data\n",
    "    for i, ((game_id, year), game_data_result) in enumerate(zip(game_batch, game_data_results)):\n",
    "        try:\n",
    "            if isinstance(game_data_result, Exception):\n",
    "                print(f\"Failed to fetch data for game {game_id}: {game_data_result}\")\n",
    "                batch_results.append((game_id, False, str(game_data_result)))\n",
    "                continue\n",
    "                \n",
    "            pbp_data, boxscore_data = game_data_result\n",
    "            \n",
    "            if not pbp_data:\n",
    "                batch_results.append((game_id, False, \"No PBP data\"))\n",
    "                continue\n",
    "            \n",
    "            # Process the data\n",
    "            processed_data, team_lineups = process_pbp_data(pbp_data, boxscore_data, game_id)\n",
    "            \n",
    "            if not processed_data:\n",
    "                batch_results.append((game_id, False, \"No processed data\"))\n",
    "                continue\n",
    "            \n",
    "            # Save to CSV\n",
    "            output_df = pd.DataFrame(processed_data)\n",
    "            filename = f\"pbp_data/{year}_{game_id}.csv\"\n",
    "            output_df.to_csv(filename, index=False)\n",
    "            \n",
    "            # Track team lineups\n",
    "            for team_id, lineups in team_lineups.items():\n",
    "                for lineup in lineups:\n",
    "                    all_team_lineups[team_id][year].add((lineup, game_id))\n",
    "            \n",
    "            batch_results.append((game_id, True, \"Success\"))\n",
    "            \n",
    "        except Exception as e:\n",
    "            batch_results.append((game_id, False, str(e)))\n",
    "    \n",
    "    return batch_results\n",
    "\n",
    "async def main_async():\n",
    "    \"\"\"Main async function\"\"\"\n",
    "    # Filter games since 2020-21 season\n",
    "    games_to_scrape = record[record['year'] >= 2021].copy()\n",
    "    print(f\"Total games in master record: {len(games_to_scrape)}\")\n",
    "    \n",
    "    # Create directories\n",
    "    os.makedirs('pbp_data', exist_ok=True)\n",
    "    os.makedirs('team_lineups', exist_ok=True)\n",
    "    \n",
    "    # Get existing games and filter\n",
    "    existing_games = get_existing_games()\n",
    "    \n",
    "    games_to_process = []\n",
    "    skipped_games = 0\n",
    "    \n",
    "    for idx, game_row in games_to_scrape.iterrows():\n",
    "        game_id = str(game_row['GAME_ID'])\n",
    "        year = game_row['year']\n",
    "        \n",
    "        if game_id in existing_games:\n",
    "            if verify_existing_file(game_id, year):\n",
    "                skipped_games += 1\n",
    "                continue\n",
    "            else:\n",
    "                games_to_process.append((game_id, year))\n",
    "        else:\n",
    "            games_to_process.append((game_id, year))\n",
    "    \n",
    "    print(f\"Skipping {skipped_games} already processed games\")\n",
    "    print(f\"Games to process: {len(games_to_process)}\")\n",
    "    \n",
    "    if len(games_to_process) == 0:\n",
    "        print(\"No new games to process!\")\n",
    "        return\n",
    "    \n",
    "    # Track results\n",
    "    all_team_lineups = defaultdict(lambda: defaultdict(set))\n",
    "    failed_games = []\n",
    "    successful_games = 0\n",
    "    \n",
    "    # Process games in batches\n",
    "    BATCH_SIZE = 20  # Process 20 games concurrently\n",
    "    MAX_CONCURRENT = 10  # Limit concurrent connections\n",
    "    \n",
    "    connector = aiohttp.TCPConnector(limit=MAX_CONCURRENT, limit_per_host=MAX_CONCURRENT)\n",
    "    timeout = aiohttp.ClientTimeout(total=60, connect=30)\n",
    "    \n",
    "    async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:\n",
    "        for i in range(0, len(games_to_process), BATCH_SIZE):\n",
    "            batch = games_to_process[i:i + BATCH_SIZE]\n",
    "            batch_num = i // BATCH_SIZE + 1\n",
    "            total_batches = (len(games_to_process) + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "            \n",
    "            print(f\"Processing batch {batch_num}/{total_batches} ({len(batch)} games)\")\n",
    "            \n",
    "            try:\n",
    "                batch_results = await process_game_batch(session, batch, all_team_lineups)\n",
    "                \n",
    "                # Count results\n",
    "                for game_id, success, message in batch_results:\n",
    "                    if success:\n",
    "                        successful_games += 1\n",
    "                        print(f\"✓ {game_id}\")\n",
    "                    else:\n",
    "                        failed_games.append(game_id)\n",
    "                        print(f\"✗ {game_id}: {message}\")\n",
    "                \n",
    "                # Small delay between batches to be respectful\n",
    "                if i + BATCH_SIZE < len(games_to_process):\n",
    "                    await asyncio.sleep(1)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing batch {batch_num}: {e}\")\n",
    "                for game_id, year in batch:\n",
    "                    failed_games.append(game_id)\n",
    "    \n",
    "    # Create team lineup index files (unchanged)\n",
    "    print(\"\\nCreating team lineup indexes...\")\n",
    "    for team_id in all_team_lineups:\n",
    "        for year in all_team_lineups[team_id]:\n",
    "            lineup_data = []\n",
    "            lineup_games = defaultdict(list)\n",
    "            \n",
    "            for lineup, game_id in all_team_lineups[team_id][year]:\n",
    "                lineup_games[lineup].append(game_id)\n",
    "            \n",
    "            for lineup, games in lineup_games.items():\n",
    "                lineup_data.append({\n",
    "                    'team_id': team_id,\n",
    "                    'year': year,\n",
    "                    'players_on': lineup,\n",
    "                    'games': '|'.join(games),\n",
    "                    'game_count': len(games)\n",
    "                })\n",
    "            \n",
    "            if lineup_data:\n",
    "                lineup_df = pd.DataFrame(lineup_data)\n",
    "                lineup_filename = f\"team_lineups/team_{team_id}_year_{year}_lineups.csv\"\n",
    "                lineup_df.to_csv(lineup_filename, index=False)\n",
    "    \n",
    "    print(f\"\\nScraping complete!\")\n",
    "    print(f\"Successfully processed: {successful_games} new games\")\n",
    "    print(f\"Skipped existing games: {skipped_games}\")\n",
    "    print(f\"Failed games: {len(failed_games)}\")\n",
    "    \n",
    "    if failed_games:\n",
    "        print(\"Failed game IDs:\")\n",
    "        for game_id in failed_games[:10]:\n",
    "            print(f\"  {game_id}\")\n",
    "        \n",
    "        failed_df = pd.DataFrame({'failed_game_ids': failed_games})\n",
    "        failed_df.to_csv('failed_games.csv', index=False)\n",
    "\n",
    "def main():\n",
    "    \"\"\"Entry point that runs the async main function\"\"\"\n",
    "    asyncio.run(main_async())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
